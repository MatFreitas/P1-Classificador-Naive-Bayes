{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nome: Matheus Freitas Sant'Ana**\n",
    "\n",
    "**Nome: Gabriel Salvator Benatar**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import emoji\n",
    "from emoji import UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperamos trabalhar no diretório\n",
      "C:\\Users\\matfs\\Desktop\\Ciência dos Dados\\P2\\P1-Classificador-Naive-Bayes\n"
     ]
    }
   ],
   "source": [
    "print('Esperamos trabalhar no diretório')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**Carregando a base de dados com os tweets classificados como relevantes e não relevantes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_excel('Uber.xlsx')\n",
    "dados['Classe'] = dados.Classe.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados['Classe'].cat.categories = ['Irrelevante', 'Relevante']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>@barb_lanzarin não!! espera que eu vou chamar ...</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>@olimpiabarbara @uber_brasil pois é, o cara só...</td>\n",
       "      <td>Relevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>eu gritando com o uber toda vez que alguma ami...</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>@paoherco1 @esmareana @fercoboen @_juliocalder...</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>peguei um uber agora e o motorista começou a t...</td>\n",
       "      <td>Relevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>thread de vídeos para usar no uber/99 quando e...</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>rt @universitariaof: e o uber que se protegeu ...</td>\n",
       "      <td>Relevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>@quebrachera eu penso no meu pai q não consegu...</td>\n",
       "      <td>Relevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>e ainda teve cara de pau de dar cartãozinho de...</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>minha mãe bebeu ousadia pra crc ontem que falo...</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Treinamento       Classe\n",
       "490  @barb_lanzarin não!! espera que eu vou chamar ...  Irrelevante\n",
       "491  @olimpiabarbara @uber_brasil pois é, o cara só...    Relevante\n",
       "492  eu gritando com o uber toda vez que alguma ami...  Irrelevante\n",
       "493  @paoherco1 @esmareana @fercoboen @_juliocalder...  Irrelevante\n",
       "494  peguei um uber agora e o motorista começou a t...    Relevante\n",
       "495  thread de vídeos para usar no uber/99 quando e...  Irrelevante\n",
       "496  rt @universitariaof: e o uber que se protegeu ...    Relevante\n",
       "497  @quebrachera eu penso no meu pai q não consegu...    Relevante\n",
       "498  e ainda teve cara de pau de dar cartãozinho de...  Irrelevante\n",
       "499  minha mãe bebeu ousadia pra crc ontem que falo...  Irrelevante"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Irrelevante    356\n",
       "Relevante      144\n",
       "Name: Classe, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados.Classe.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há muito mais tweets irrelevantes do que relevantes. Vamos tentar deixar a quantidade mais ou menos parecida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_irr = dados.loc[dados.Classe == 'Irrelevante', :]\n",
    "dados_irr = dados_irr.loc[:207, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Irrelevante    144\n",
       "Relevante        0\n",
       "Name: Classe, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_irr.Classe.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_r = dados.loc[dados.Classe == 'Relevante', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Relevante      144\n",
       "Irrelevante      0\n",
       "Name: Classe, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_r.Classe.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificador automático de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O produto de que selecionamos foi a Uber Technologies, Inc., mais conhecida como Uber. É uma empresa multinacional americana que oferece diversos serviços, serviço de carona, entrega de comida e um sistema de mobilidade de patinetes e bicicletas (mais recente), mas para nosso projeto nos focamos mais no serviço de carona. \n",
    "\n",
    "Para classificarmos nosso excel, no primero momento, para verificar o que era relevante e o que não era, lemos todos e decidimos critérios, depois relemos e aplicamos nossos critérios de avaliação em cada tweet para verificar qual era relevante. Como primeiro critério decidimos que os _tweets_ de reclamação e de elogios em relação ao Uber (aplicativo) eram relevantes, depois decidimos que as reclamações em relação ao motorista e a avaliação de passageiros também eram relevantes, pois fazem parte da funcionalidade do aplicativo. Dentre os _tweets_ nós selecionamos como não relevantes aqueles que tinham relação com UberEats (outro serviço da Uber), e na maioria dos casos comentários feitos que não tinham relação com o aplicativo em si. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função que limpa a pontuação e outros símbolos irrelevantes para a classificação dos _tweets_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "\n",
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        Função de limpeza muito simples que troca alguns sinais básicos por espaços\n",
    "    \"\"\"\n",
    "    #import string\n",
    "    punctuation = '[!-.:?;@_]' # Note que os sinais [] são delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    \n",
    "    # Proposta de melhoria do classificador: remover palavras comuns que não \n",
    "    # afetam na relevância de uma frase.\n",
    "    words =  [' que ', ' e ', ' rt ', ' de ', ' do ', ' da ', ' uma ', ' um ', ' aquela ', ' aquele ', ' aqueles ', ' aquelas ']\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        pattern = re.compile(words[i])\n",
    "        text_subbed = re.sub(pattern, ' ', text_subbed)\n",
    "        \n",
    "    return text_subbed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Limpando os _tweets_ do _DataFrame_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matfs\\Downloads\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dados['Treinamento'])):\n",
    "    dados['Treinamento'][i] = cleanup(dados['Treinamento'][i].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pior coisa é ouvir gente privilegiada anda car...</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eu morrendo calor o uber geladinho tudo q eu q...</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to lembrando aqui dia em a mairine a nathália ...</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lucasp711 to cortando o cabelo  se ainda tive...</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt  jungsbloomie  acabei o dinheiro semana c i...</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento       Classe\n",
       "0  pior coisa é ouvir gente privilegiada anda car...  Irrelevante\n",
       "1  eu morrendo calor o uber geladinho tudo q eu q...  Irrelevante\n",
       "2  to lembrando aqui dia em a mairine a nathália ...  Irrelevante\n",
       "3   lucasp711 to cortando o cabelo  se ainda tive...  Irrelevante\n",
       "4  rt  jungsbloomie  acabei o dinheiro semana c i...  Irrelevante"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Criando os _DataFrames_ das categorias de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_irr = dados.loc[dados.Classe == 'Irrelevante', :]\n",
    "dados_irr = dados_irr.loc[:209, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_r = dados.loc[dados.Classe == 'Relevante', :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Aqui corrigimos o index de cada _DataFrame_ (eles tinham herdado o index do _DataFrame_ total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dados_r.index.values)):\n",
    "    dados_r.index.values[i] = i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dados_irr.index.values)):\n",
    "    dados_irr.index.values[i] = i "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Criando uma lista com as palavras de cada _tweet_ do _DataFrame_ dos irrelevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pior', 'coisa', 'é', 'ouvir', 'gente', 'privilegiada', 'anda', 'carro', 'todo', 'dia', 'seja', 'uber', 'ou', 'carro', 'pai', 'vai', 'ter', 'andar', 'ônibus', 'realmente', 'essas', 'pessoas', 'precisam', 'amadurecer'], ['eu', 'morrendo', 'calor', 'o', 'uber', 'geladinho', 'tudo', 'q', 'eu', 'queria']]\n"
     ]
    }
   ],
   "source": [
    "todas_palavras_irrelevante = []\n",
    "for i in range(len(dados_irr.Treinamento)):\n",
    "    todas_palavras_irrelevante.append(dados_irr['Treinamento'][i].split())\n",
    "print(todas_palavras_irrelevante[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa lista é composta de várias outras listas, em que cada uma delas corresponde às palavras de certo _tweet_. Vamos fazer com que a lista de palavras seja apenas UMA grande lista composta apenas pelas _strings_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pior', 'coisa', 'é', 'ouvir', 'gente', 'privilegiada', 'anda', 'carro', 'todo', 'dia', 'seja', 'uber', 'ou', 'carro', 'pai', 'vai', 'ter', 'andar', 'ônibus', 'realmente', 'essas', 'pessoas', 'precisam', 'amadurecer', 'eu', 'morrendo', 'calor', 'o', 'uber', 'geladinho', 'tudo', 'q', 'eu', 'queria', 'to']\n"
     ]
    }
   ],
   "source": [
    "todas_palavras_irr = []\n",
    "for i in range(len(todas_palavras_irrelevante)):\n",
    "    for j in range(len(todas_palavras_irrelevante[i])):\n",
    "        todas_palavras_irr.append(todas_palavras_irrelevante[i][j])\n",
    "print(todas_palavras_irr[:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tornar essa lista em um _DataFrame_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     pior\n",
       "1    coisa\n",
       "2        é\n",
       "3    ouvir\n",
       "4    gente\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serie_Irr = pd.Series(todas_palavras_irr)\n",
    "serie_Irr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo o processo feito acima é repetido para o _DataFrame_ dos _tweets_ relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "todas_palavras_relevante = []\n",
    "for i in range(len(dados_r.Treinamento)):\n",
    "    todas_palavras_relevante.append(dados_r['Treinamento'][i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', 'leandra', 'bdz', 'meninas', 'uber']\n"
     ]
    }
   ],
   "source": [
    "todas_palavras_r = []\n",
    "for i in range(len(todas_palavras_relevante)):\n",
    "    for j in range(len(todas_palavras_relevante[i])):\n",
    "        todas_palavras_r.append(todas_palavras_relevante[i][j])\n",
    "print(todas_palavras_r[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         rt\n",
       "1    leandra\n",
       "2        bdz\n",
       "3    meninas\n",
       "4       uber\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serie_r = pd.Series(todas_palavras_r)\n",
    "serie_r.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Fazendo as tabelas com as contagens absolutas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uber    146\n",
       "o        77\n",
       "a        53\n",
       "no       46\n",
       "eu       45\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabela_r = serie_r.value_counts()\n",
    "tabela_r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uber    147\n",
       "o        81\n",
       "eu       52\n",
       "a        44\n",
       "no       41\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabela_irr = serie_Irr.value_counts()\n",
    "tabela_irr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Fazendo as tabelas com as contagens relativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uber    5.52\n",
       "o       2.91\n",
       "a       2.00\n",
       "no      1.74\n",
       "eu      1.70\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabela_relativa_r = (serie_r.value_counts(True)*100).round(2)\n",
    "tabela_relativa_r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uber    6.02\n",
       "o       3.32\n",
       "eu      2.13\n",
       "a       1.80\n",
       "no      1.68\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabela_relativa_irr = (serie_Irr.value_counts(True)*100).round(2)\n",
    "tabela_relativa_irr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Fazendo todo o processo acima só que para o _DataFrame_ total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "todas_palavras_v1 = []\n",
    "for i in range(len(dados.index.values)):\n",
    "    todas_palavras_v1.append(dados.Treinamento[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pior', 'coisa', 'é', 'ouvir', 'gente']\n"
     ]
    }
   ],
   "source": [
    "todas_palavras = []\n",
    "for i in range(len(todas_palavras_v1)):\n",
    "    for j in range(len(todas_palavras_v1[i])):\n",
    "        todas_palavras.append(todas_palavras_v1[i][j])\n",
    "print(todas_palavras[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     pior\n",
       "1    coisa\n",
       "2        é\n",
       "3    ouvir\n",
       "4    gente\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serie_todas = pd.Series(todas_palavras)\n",
    "serie_todas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uber    500\n",
       "o       272\n",
       "eu      167\n",
       "a       164\n",
       "no      150\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabela_todas = serie_todas.value_counts()\n",
    "tabela_todas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uber    5.42\n",
       "o       2.95\n",
       "eu      1.81\n",
       "a       1.78\n",
       "no      1.63\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabela_relativa_todas = (serie_todas.value_counts(True)*100).round(2)\n",
    "tabela_relativa_todas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Função do classificador Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(frase, tabela_relativa_r, tabela_relativa_irr):\n",
    "    \n",
    "    # Filtrando a frase tirando pontuação e símbolos irrelevantes\n",
    "    frase = cleanup(frase.lower())\n",
    "    \n",
    "    # Separando as palavras da frase e colocando-as em uma lista.\n",
    "    lista_palavras = frase.split()\n",
    "\n",
    "    # Quando uma palavra  e um emoji estão colados, separá-los para que sejam \n",
    "    # considerados palavras diferentes.\n",
    "    for k in range(len(lista_palavras)):\n",
    "        palavra = lista_palavras[k]\n",
    "        if len(palavra) > 5 and palavra in lista_palavras:\n",
    "            for i in range(len(palavra)-1):\n",
    "                if palavra[i] not in UNICODE_EMOJI and palavra[i+1] in UNICODE_EMOJI:\n",
    "                    a = \" \".join(palavra[i:i+2])\n",
    "                    b = palavra.replace(palavra[i:i+2], a)\n",
    "                    b = b.split()\n",
    "                    lista_palavras.remove(lista_palavras[k])\n",
    "                    for m in b:\n",
    "                        lista_palavras.append(m)\n",
    "\n",
    "    # Quando há o uso de emojis e eles estão 'colados', separá-los para que sejam\n",
    "    # considerados palavras diferentes.\n",
    "    k = 0\n",
    "    for m in range(len(lista_palavras)):\n",
    "        for i in lista_palavras[m]:\n",
    "            if i in UNICODE_EMOJI:\n",
    "                k += 1\n",
    "        if k == len(lista_palavras[m]):\n",
    "            sub = \",\".join(lista_palavras[m])\n",
    "            a = sub.split(\",\")\n",
    "            lista_palavras.remove(lista_palavras[m])\n",
    "            for s in range(len(a)):\n",
    "                lista_palavras.append(a[s])\n",
    "        k = 0\n",
    "        \n",
    "        \n",
    "    # Contando o total de palavras em cada categoria com as contagens absolutas.\n",
    "    totalR = tabela_r.value_counts().sum()\n",
    "    totalIrr = tabela_irr.value_counts().sum()\n",
    "    total = totalR + totalIrr\n",
    "    \n",
    "    # P(R)\n",
    "    probR = totalR/total\n",
    "    \n",
    "    # P(Rc)\n",
    "    probIrr=totalIrr/total\n",
    "    \n",
    "    # P(frase|R)\n",
    "    probFraseDadoR = 100\n",
    "    for i in lista_palavras:\n",
    "        if i in tabela_relativa_r:\n",
    "            probFraseDadoR *= (tabela_relativa_r['{}'.format(i)] + 1)/(totalR + total)\n",
    "        else:\n",
    "            # Aqui, fazemos o Laplace Smoothing, pois nosso classificador não tem\n",
    "            # essa palavra em seu 'vocabulário'.\n",
    "            probFraseDadoR *= 1/(totalR + total)\n",
    "    \n",
    "    # P(frase|Rc)\n",
    "    probFraseDadoIrr = 100\n",
    "    for i in lista_palavras:\n",
    "        if i in tabela_relativa_irr:\n",
    "             probFraseDadoIrr *= (tabela_relativa_irr['{}'.format(i)] + 1)/(totalIrr + total)\n",
    "        else:\n",
    "            # Aqui, fazemos o Laplace Smoothing novaamente, só que para os irrelevantes.\n",
    "            probFraseDadoIrr *= 1/(totalIrr + total)\n",
    "    \n",
    "    # P(R|frase)\n",
    "    probRDadoFrase = probFraseDadoR*probR\n",
    "    \n",
    "    # P(Rc|frase)\n",
    "    probIrrDadoFrase = probFraseDadoIrr*probIrr\n",
    "    \n",
    "    if probRDadoFrase > probIrrDadoFrase:\n",
    "        return 'Relevante'\n",
    "    else:\n",
    "        return 'Irrelevante'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Demonstração de que o código do naive-bayes de fato separa palavra de emoji e emoji de emoji (apenas se copiou o código da função do naive-bayes que é responsável por isso e testou-se com uma frase aleatória  do documento _Uber.xlsx_ que contenha essas condições). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['minha', 'mãe', 'bebeu', 'ousadia', 'pra', 'crc', 'ontem', 'falou', 'pra', 'eu', 'pedir', 'uber', 'com', 'copo', 'descartável', '🤦', '🤣', '🤣', '🤣']\n"
     ]
    }
   ],
   "source": [
    "frase = \"minha mãe bebeu ousadia pra crc ontem que falou pra eu pedir uber com copo descartável🤦🤣🤣🤣\"\n",
    "frase = cleanup(frase.lower())\n",
    "lista_palavras = frase.split()\n",
    "for k in range(len(lista_palavras)):\n",
    "    palavra = lista_palavras[k]\n",
    "    for i in range(len(palavra)-1):\n",
    "        if palavra[i] not in UNICODE_EMOJI and palavra[i+1] in UNICODE_EMOJI:\n",
    "            a = \" \".join(palavra[i:i+2])\n",
    "            b = palavra.replace(palavra[i:i+2], a)\n",
    "            b = b.split()\n",
    "            lista_palavras.remove(palavra)\n",
    "            for m in b:\n",
    "                lista_palavras.append(m)\n",
    "k = 0\n",
    "for m in range(len(lista_palavras)):\n",
    "    for i in lista_palavras[m]:\n",
    "        if i in UNICODE_EMOJI:\n",
    "            k += 1\n",
    "    if k == len(lista_palavras[m]):\n",
    "        sub = \",\".join(lista_palavras[m])\n",
    "        a = sub.split(\",\")\n",
    "        lista_palavras.remove(lista_palavras[m])\n",
    "        for s in range(len(a)):\n",
    "            lista_palavras.append(a[s])\n",
    "    k = 0\n",
    "\n",
    "print(lista_palavras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance do Classificador\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Lendo o documento na aba **Testes** e configurando os tipos dos dados das colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_teste = pd.read_excel('Uber.xlsx', sheet_name = \"Teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_teste['Classe'] = dados_teste.Classe.astype('category')\n",
    "dados_teste['Classe'].cat.categories = ['Irrelevante', 'Relevante']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Criando uma nova coluna chamada **Bayes** que é inicializada com todos seus valores sendo 0. Depois faz um for para que o naive-bayes classifique cada frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matfs\\Downloads\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\matfs\\Downloads\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "dados_teste['Bayes'] = 0\n",
    "\n",
    "for i in range(len(dados_teste.index)):\n",
    "    dados_teste['Bayes'][i] = naive_bayes(dados_teste['Teste'][i], tabela_relativa_r, tabela_relativa_irr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>Classe</th>\n",
       "      <th>Bayes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@mirtuda ri muuito no uber lembrando 😂😂😂\\ndo n...</td>\n",
       "      <td>Irrelevante</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>meu deus o uber que meu amigo pegou coitado ht...</td>\n",
       "      <td>Relevante</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@guga15_ uber mais perfeito não exis....</td>\n",
       "      <td>Irrelevante</td>\n",
       "      <td>Relevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mano kkkkkkkkkk 1x0 pro uber pqp https://t.co/...</td>\n",
       "      <td>Irrelevante</td>\n",
       "      <td>Irrelevante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>peguei um uber todo tatuado e escultando um ra...</td>\n",
       "      <td>Relevante</td>\n",
       "      <td>Relevante</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Teste       Classe        Bayes\n",
       "0  @mirtuda ri muuito no uber lembrando 😂😂😂\\ndo n...  Irrelevante  Irrelevante\n",
       "1  meu deus o uber que meu amigo pegou coitado ht...    Relevante  Irrelevante\n",
       "2           @guga15_ uber mais perfeito não exis....  Irrelevante    Relevante\n",
       "3  mano kkkkkkkkkk 1x0 pro uber pqp https://t.co/...  Irrelevante  Irrelevante\n",
       "4  peguei um uber todo tatuado e escultando um ra...    Relevante    Relevante"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_teste.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Aqui se calcula a porcentagem de acertos na relevância e na irrelevância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Irrelevante    113\n",
       "Relevante       87\n",
       "Name: Bayes, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_teste.Bayes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Irrelevante    159\n",
       "Relevante       41\n",
       "Name: Classe, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_teste.Classe.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acertos relevantes: 23\n",
      "Erros relevantes: 18\n",
      "Acertos irrelevantes: 95\n",
      "Erros irrelevantes: 64\n"
     ]
    }
   ],
   "source": [
    "acertos_relevantes = 0\n",
    "acertos_irrelevantes = 0\n",
    "erros_relevantes = 0\n",
    "erros_irrelevantes = 0\n",
    "total = 200\n",
    "\n",
    "for i in range(len(dados_teste.index)):\n",
    "    if dados_teste.Classe[i] == dados_teste.Bayes[i] and dados_teste.Classe[i] == 'Irrelevante':\n",
    "        acertos_irrelevantes += 1\n",
    "    elif dados_teste.Classe[i] == dados_teste.Bayes[i] and dados_teste.Classe[i] == 'Relevante':\n",
    "        acertos_relevantes += 1\n",
    "    elif dados_teste.Classe[i] != dados_teste.Bayes[i] and dados_teste.Classe[i] == 'Irrelevante':\n",
    "        erros_irrelevantes += 1\n",
    "    elif dados_teste.Classe[i] != dados_teste.Bayes[i] and dados_teste.Classe[i] == 'Relevante':\n",
    "        erros_relevantes += 1\n",
    "        \n",
    "print(\"Acertos relevantes: {}\".format(acertos_relevantes))\n",
    "print(\"Erros relevantes: {}\".format(erros_relevantes))\n",
    "print(\"Acertos irrelevantes: {}\".format(acertos_irrelevantes))\n",
    "print(\"Erros irrelevantes: {}\".format(erros_irrelevantes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentagem Acertos (Relevância): 56.09756097560976\n",
      "Porcentagem Acertos (Irrelevância): 59.74842767295597\n"
     ]
    }
   ],
   "source": [
    "porcentagem_relevancia = acertos_relevantes/(acertos_relevantes+erros_relevantes)\n",
    "porcentagem_irrelevancia = acertos_irrelevantes/(acertos_irrelevantes+erros_irrelevantes)\n",
    "\n",
    "print(\"Porcentagem Acertos (Relevância): {}\".format(porcentagem_relevancia*100))\n",
    "print(\"Porcentagem Acertos (Irrelevância): {}\".format(porcentagem_irrelevancia*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentagem de verdadeiros positivos: 11.5\n",
      "Porcentagem de falsos positivos: 9.0\n",
      "Porcentagem de verdadeiros_negativos: 47.5\n",
      "Porcentagem de falsos negativos: 32.0\n"
     ]
    }
   ],
   "source": [
    "total = acertos_relevantes + erros_relevantes + acertos_irrelevantes + erros_irrelevantes\n",
    "\n",
    "verdadeiros_positivos = acertos_relevantes/total\n",
    "falsos_positivos = erros_relevantes/total\n",
    "verdadeiros_negativos = acertos_irrelevantes/total\n",
    "falsos_negativos = erros_irrelevantes/total\n",
    "\n",
    "print(\"Porcentagem de verdadeiros positivos: {}\".format((verdadeiros_positivos*100)))\n",
    "print(\"Porcentagem de falsos positivos: {}\".format((falsos_positivos*100)))\n",
    "print(\"Porcentagem de verdadeiros_negativos: {}\".format((verdadeiros_negativos*100)))\n",
    "print(\"Porcentagem de falsos negativos: {}\".format((falsos_negativos*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Concluindo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O nosso classficiador acabou com uma acurácia positiva tanto para _tweets_ relevantes como irrelevantes. Porém, ela não ficou tão alta quanto esperávamos. Isso pode ter acontecido devido aos seguintes fatos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Em nossa classificação e filtragem durante o projeto tiveram alguns fatores que acabaram atrapalhando nossos resultados, como por exemplo porcentagem de acertos e erros que o nosso programa calculou. Um desses fatores era o fato de que durante a classificação haviam várias palavras que ao mesmo tempo eram usadas para classificar um tweet relevante ou irrelevante, um exemplo disso: “Nossa o motorista era um gato”, esse tweet seria classificado como irrelevante pois não têm relação com o aplicativo ou a viagem. Já um tweet como: “Ótimo motorista, fez um ótimo trajeto”, seria classificado como relevante pois se trata do comportamento de um usuário do aplicativo e afeta a imagem do aplicativo de certo modo. Em ambos casos a palavra “motorista” foi usada, porém em contextos diferentes, isso na hora da filtragem faz com que a palavra motorista quando aparecer releve um tweet como positivo ou negativo falsamente aumentando no número de falsos positivos e negativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Outro fator muito importante que acabou nos atrapalhando muito foi o Covid-19,  na primeira vez que geramos os tweets vieram 300 tweets na aba treinamento e apenas 80 na parte de testes, enquanto eram necessários 200. Devido ao problema de que não houveram tweets suficiente decidimos gerar outro documento, e aproveitamos essa oportunidade também para pegar mais _tweets_ da nossa base de treinamento e deixar o vocabulário do classificador mais rico. O problema foi que o Covid-19 já tinha começado a se espalhar e consequentemente quase todos os tweets se tratavam dele. Pelo fato de que quase todos os tweets que se tratavam do coronavírus eram irrelevantes acabou causando uma falta de vocabulário em nossa base de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Melhorias e como implementá-las"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A primeira melhoria clara que poderia ter sido feita (como já foi 'pincelada' acima) para esse classificador é desconsiderar uma presunção que fizemos: quando classifica uma frase e por conseguinte cada palavra individual dela, assumimos que elas eram **independentes**, ou seja, que a ordem das palavras na frase não importava. Porém, realisticamente, isso não é verdade, pois em casos como por exemplo na frase \"Que motorista péssimo!\" e \"O motorista estranhou mas era porque estava me sentindo péssimo!\", a palavra péssimo tem o mesmo peso em cada frase, mas enquanto que na primeira, ela está associada ao motorista, o que seria de grande relevância para a empresa Uber (como foi explicado nos critérios de classificação), na segunda não tem tanta relevância assim, pois está relacionado aum problema pessoal do usuário. O **modo** de implementar isso mais claro seria implementar _N-grams_, em que são definidos _tokens_ compostos por sequências de palavras, cuja divisão pode ser feita sendo cada palavra do token composta por uma palavra (_unigram_) ou cada palavra é composta por duas (_bigram_) e assim por diante. (Referência: [A practical explanation of a Naive Bayes classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A segunda melhoria que poderia ter sido feita é a de agrupar variações de uma mesma palavra como sendo apenas uma para o nosso classificador analisar. Isto é, um tema recorrente nos _tweets_ relevantes era sobre o pagamento, porém, a forma como os usuários discorriam sobre isso fazia com que as palavras utilizadas fossem diferentes para cada situação devido à própria natureza da frase. Assim, enquanto que em alguns _tweets_ utilizado \"Paguei com o cartão\" , em outros era utilizado \"Não sabia como fazer o pagamento\", e, as palavras \"paguei\" e \"pagamento\" têm o mesmo lema. Para implementar isso, seria necessário utilizar um **método chamado Lematização**, que engloba palavras que têm o mesmo lema como sendo uma única palavra, o que ajudaria o nosso classificador a definir melhor a relevância de uma palavra. O jeito mais fácil de fazer isso é pesquisando no dicionário mesmo. (Referência: [Lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Por que não posso usar o classificador para gerar mais amostras de treinamento?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma das maiores desvantagens de utilizar o Naive-Bayes como classificador é a forma como ele aprende a classificar informações. O problema disso é que ele não aprende o _pattern_/estrutura dps _tweets_, mas que apenas lembra as palavras da base de treinamento. Por isso fazemos o _Smoothing_ por exemplo, para que quando há uma palavra que não se encontra no vocabulário de nosso classificador, adicionamos um valor que faça com que ele não zere a probabilidade da frase. Resumindo, o Naive-Bayes carece do que redes neurais tem, por exemplo. Por isso, não faz muito senitdo o Naive-Bayes gerar automaticamente conjuntos de treinamento; ele não consegue **visualizar relações**. O _Twitter_, por exemplo, usa uma API chamada _Sentiment140_ que utiliza uma base de dados já existente para coletar instâncias sobre a relação que se deseja analisar e as utiliza para gerar automaticamente amostras de treinamento (Referência: [AnaliseSentimentosExpressos.pdf](https://repositorio.ufu.br/bitstream/123456789/20133/1/AnaliseSentimentosExpressos.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferentes Cenários para o Naive-Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em nosso projeto usamos o classificador de Naive Bayes para classificar tweets de uma empresa como relevantes ou irrelevantes, mas isso foi só um dos casos, ele têm várias outras aplicações que são muito usadas hoje em dia. Uma delas é para classificar se um email vai ou não para caixa de spam, por ele ser muito usado para classificações de texto, ele encaixa muito bem na hora de mandar um email para caixa de spam. O Naive Bayes quando usado em conjunto com outros programas e classificadores pode ser muito bom para um sistema de recomendação, como por exemplo a página inicial do Youtube de recomendados para você, ou a Netfilx, e como um terceiro exemplo, já que ele é um otimo classificador de texto como dito anterior a resolução de problemas relacionadas a textos também é muito bom."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
